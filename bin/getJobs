#!/usr/bin/python3

#
# get all jobs run from filter
# get the session ids for those analyses
# Get the flywheel paths for those sessions sorting by group/project

import argparse
import csv
import flywheel
import fwgearutils
import json
import os
import re
import sys

# *** Need to add comments to suss out techdev scans

from collections import OrderedDict

CmdName = os.path.basename(sys.argv[0])

def progress(count, total, status=''):
    bar_len = 60
    filled_len = int(round(bar_len * count / float(total)))

    percents = round(100.0 * count / float(total), 1)
    bar = '=' * filled_len + '-' * (bar_len - filled_len)

    sys.stderr.write('%s/%s [%s] %s%s ...%s\r' % (i, total, bar, percents, '%', status))
    sys.stderr.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)

# *** nice to have job duration, cpus,  cpu_sec, where it ran, 

InitialSessionAnalysis = {}

def isInitialAnalysis(job_id, session_id=None, debug=False):
    if (session_id in InitialSessionAnalysis.keys()):
        return(job_id == InitialSessionAnalysis[session_id])

    try:
        jobs = fw.jobs.find("_id={}".format(job_id), limit=100000)
    except (flywheel.rest.ApiException) as e:
        print("job_id {} not found".format(job_id), file=sys.stderr)
        return(False)

    if (len(jobs)):
        job = jobs[0].reload()

        if (debug):
            print("job.id = {}".format(job.id), file=sys.stderr)

        detail = fw.get_job_detail(job.id)
        session_id = detail.parent_info.session.id

        if (debug):
            print("session_id = {}".format(session_id), file=sys.stderr)

        try:
            session = fw.get(session_id)
        except (flywheel.rest.ApiException) as e:
            InitialSessionAnalysis[session_id] = None
            print("job_id({})->session.id({}) not found".format(job.id,session_id), file=sys.stderr)
            return(False)

        analyses = sorted(session.analyses, key=lambda a: a.created)
        if (len(analyses)):
            InitialSessionAnalysis[session_id] = analyses[0].job.id

            if (job_id == analyses[0].job.id):
                if (debug):
                    print("match {} = {}".format(job_id, analyses[0].job.id), file=sys.stderr)

                return(True)

            else:
                if (debug):
                    print("no match {} = {}".format(job_id, analyses[0].job.id), file=sys.stderr)

                    for a in analyses:
                        print(a.job.id, file=sys.stderr)
                return(False)

    return(None)
     
def initRow(
	job_date=None,
	job_id=None,
        job_elapsed_time_ms=None,
        job_runtime_ms=None,
	gear_id=None,
	gear_name=None,
	gear_version=None,
	gear_category=None,
	job_origin_id=None,
	job_origin_type=None,
	job_state=None,
        job_cpus=None,
	initial_analysis=None,
        group=None,
        project=None,
        subject=None,
        session=None,
        session_id=None,
	acquisition_name=None,
	acquisition_id=None,
    ):

    row = OrderedDict([
	( 'job_date', job_date ),
	( 'job_id', job_id ),
	( 'gear_id', gear_id ),
	( 'gear_name', gear_name ),
	( 'gear_version', gear_version ),
	( 'gear_category', gear_category ),
	( 'job_origin_id', job_origin_id ),
	( 'job_origin_type', job_origin_type ),
	( 'job_state', job_state ),
        ( 'job_elapsed_time_ms', job_elapsed_time_ms ), 
        ( 'job_runtime_ms', job_runtime_ms ), 
        ( 'job_cpus', job_cpus), 
	( 'initial_analysis', initial_analysis ),
        ( 'group', group ),
        ( 'project', project ),
        ( 'subject', subject ),
        ( 'session', session ),
        ( 'session_id', session_id ),
	( 'acquisition_name', acquisition_name ),
	( 'acquisition_id', acquisition_id ),
    ])
    return(row)


ap = argparse.ArgumentParser()

ap.add_argument('-b', '--bar-length', action='store', default=40, type=int, help='bar length')
ap.add_argument('-e', '--exhaustive', action='store_false', default=True, help='exhaustive session flag')
ap.add_argument('-i', '--initial-analysis', action='store_true', help='check for initial analysis flag')
ap.add_argument('-l', '--limit', action='store', default=1000000, type=int, help='Flywheel cost allocation csv file')
ap.add_argument('-p', '--progressbar', action='store_true', help='show progress bar')
ap.add_argument('-v', '--verbose', action='store_true', help='verbose')
ap.add_argument('filter', nargs='*', type=str, default=None, help='arguments to fw.find()')

args = ap.parse_args()

UIDs2Labels = {}

fw = fwgearutils.getFW(args, Root=True)

reapers = fw.get_all_devices()
for r in reapers:
    UIDs2Labels[r.id] = r.name

Gears = {}
gears = fw.get_all_gears()
for g in gears:
    UIDs2Labels[g.id] = g.gear.name
    Gears[g.id] = g

projects = fw.get_all_projects(exhaustive=args.exhaustive, limit=args.limit)
for p in projects:
    UIDs2Labels[p.id] = p.label

subjects = fw.get_all_subjects(exhaustive=args.exhaustive, limit=args.limit)
for s in subjects:
    UIDs2Labels[s.id] = s.label

filter=','.join(args.filter)
print("filter = filter={}".format(filter), file=sys.stderr)
jobs = []
if (len(args.filter)):
#  jobs = fw.get_all_jobs(limit=args.limit, filter=filter)
  j = fw.jobs.iter_find(*args.filter)
  for i in j:
      jobs.append(i)
else:
  jobs = fw.get_all_jobs(limit=args.limit)

print("Len jobs = ",len(jobs), file=sys.stderr)

l = len(jobs)
i = 0

header = True
out = initRow()
writer = csv.DictWriter(sys.stdout, out.keys(),lineterminator='\n' )

Sessions = {}

for job in jobs:
    job = job.reload()

    if (args.progressbar):
        progress(i, l)
        i += 1

    detail = fw.get_job_detail(job.id)
    if (args.verbose):
        print("job", job, "detail", detail, file=sys.stderr)

    try:
        job_id = job.id
        gear_id = job.gear_id
        job_origin_id = job.origin.id

        
        if (getattr(detail.parent_info, 'session') and getattr(detail.parent_info.session, 'id')):
            session_id = detail.parent_info.session.id
        else:
            session_id = None

        if (getattr(detail.parent_info, 'acquisition') and getattr(detail.parent_info.acquisition, 'id')):
            acquisition_id = detail.parent_info.acquisition.id
        else:
            acquisition_id = None
            # print("detail missing acquisition info", detail, file=sys.stderr)

        try:
            cores = job['profile']['executor']['cpu_cores']
        except (AttributeError,TypeError) as e:
            cores = None

        if (args.initial_analysis):
            InitialAnalysis = isInitialAnalysis(job.id, session_id=session_id)
        else:
            InitialAnalysis = None

        out = initRow(
            job_date=job.created,
            job_id=job.id,
            gear_id=job.gear_id,
            gear_name=job.gear_info.name,
            gear_version=job.gear_info.version,
            gear_category=job.gear_info.category,
            job_origin_id=job.origin.id,
            job_origin_type=job.origin.type,
            job_state=job.state,
            job_elapsed_time_ms=job['profile']['elapsed_time_ms'],
            job_runtime_ms=job['profile']['total_time_ms'],
            job_cpus=cores,
            initial_analysis=InitialAnalysis,
            group=detail.parent_info.group.id,  #ID is one word lowercase, label can be words and mixed case
            project=detail.parent_info.project.label,
            subject=detail.parent_info.subject.label,
            session=detail.parent_info.session.label,
            session_id=detail.parent_info.session.id,
            acquisition_id=acquisition_id
        )

    except (AttributeError) as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        print(exc_type, fname, exc_tb.tb_lineno, file=sys.stderr)

    if (header):
        writer.writeheader()
        header = False

    writer.writerow(out)

#    if (args.verbose):
#        print(json.dumps(fwgearutils.sloppyCopy(session, recurse=True), indent=2), file=sys.stderr)



