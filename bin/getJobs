#!/usr/bin/python3

#
# get all jobs run from filter
# get the session ids for those analyses
# Get the flywheel paths for those sessions sorting by group/project

#
# you can run as:
#  getJobs -v -d -aei 'created>2022-04-28' 'created<2022-04-29' group=dwolklab
# to narrow down the scope of jobs being considered
#

import argparse
import csv
import flywheel
import fwgearutils
import json
import os
import pyjq
import re
import sys
import yaml

# *** Need to add comments to suss out techdev scans

from collections import OrderedDict
from dateutil import parser

CmdName = os.path.basename(sys.argv[0])
Epoch = parser.parse('2020-01-01')

def progress(count, total, status=''):
    bar_len = 60
    filled_len = int(round(bar_len * count / float(total)))

    percents = round(100.0 * count / float(total), 1)
    bar = '=' * filled_len + '-' * (bar_len - filled_len)

    sys.stderr.write('%s/%s [%s] %s%s ...%s\r' % (i, total, bar, percents, '%', status))
    sys.stderr.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)

# *** nice to have job duration, cpus,  cpu_sec, where it ran, 

InitialSessionAnalysis = {}

def isSessionReaped(session_id=None, debug=False):
    session = fw.get(session_id)
    for a in session.acquisitions():
        try:
            acquisition_origin_id = a.files[0].origin.id
            if (acquisition_origin_id in UIDs2Labels.keys()):
                if (debug):
                    print("id {} from {}".format(acquisition_origin_id, UIDs2Labels[acquisition_origin_id]), file=sys.stderr)
                return(True)

        except (AttributeError) as e:
            True

    return(False)
    return(False)
    
#
# category types seem to be:
# analysis
# classifier
# converter
# qa
#
def isAnalysisJob(job):
    JobCategory = None

    try:
        JobCategory = job.gear_info.category
    except (AttributeError) as e:
        print("{}: Job {} has no gear_info.category".format(e, job.id), file=sys.stderr)

    return(JobCategory == "analysis")

def getPostEpochAcquisitionIds(job=None, debug=False):
    InputAcquisitionIds = []
    PostEpochAcquisitionIds = {}

    if (job.config['inputs'] == None):
        inputs = []
    elif (type(job.config['inputs']) == dict):
        inputs = [ job.config['inputs'] ]
    else:
        inputs = job.config['inputs']

    for i in inputs:
        InputAcquisitionIds.extend(pyjq.all('..|select(.type == "acquisition")? | .id',fwgearutils.sloppyCopy(i)))

    for aid in InputAcquisitionIds:
        try:
            a = fw.get(aid)
        except (flywheel.rest.ApiException) as e:
            print("{}: No such acquisition {} in job {}".format(e, aid, job.id), file=sys.stderr)
            continue

        a = a.reload()

        try:
            AcquisitionDate = parser.parse(str(a.files[0].info['AcquisitionDate']))
            if (AcquisitionDate > Epoch):
                PostEpochAcquisitionIds[aid] = AcquisitionDate
            else:
                if (debug):
                    print("AcqusitionDate for {} of Job {} is {} before the Epoch {}".format(
                        aid,
                        job.id,
                        AcquisitionDate,
                        Epoch), file=sys.stderr)

        except (KeyError) as e:
            print("{}: Acquisition {} of job {} has no AcquisitionDate".format(e, aid, job.id), file=sys.stderr)

    return(PostEpochAcquisitionIds)

def acquisitionId2SessionId(AcquisitionId):
    Acquisition = fw.get(AcquisitionId)
    return(Acquisition.parents.session)

def isJobInitialSessionAnalysis(JobId=None, SessionId=None, debug=False):
    
    session_id = SessionId
    session = fw.get_session(SessionId)

    if (debug):
        print("isJobInitialSessionAnalysis checking '{}'".format(session_id), file=sys.stderr)

    if (debug):
        print("  type(InitialSessionAnalysis) = {}".format(type(InitialSessionAnalysis)), file=sys.stderr)

    if (session_id in InitialSessionAnalysis.keys()):
        if (debug):
            print("  session_id({}) in InitialSessionAnalysis".format(session_id), file=sys.stderr)

        if (JobId == InitialSessionAnalysis[session_id]):
            if (debug):
                print("{} already in InitialSessionAnalysis".format(session_id), file=sys.stderr)

            return(True)

    if (debug):
        print("  session_id({}) not in InitialSessionAnalysis".format(session_id), file=sys.stderr)

    analyses = sorted(session.analyses, key=lambda a: a.created)

    if (debug):
        print("  analyses len = {}".format(len(analyses)), file=sys.stderr)

    if (len(analyses)):
        if (debug):
            stuff = []
            for a in analyses:
                stuff.append({'id': a.job.id, 'date': str(a.created)})
            print("session({}).analyses = {}".format(session_id,stuff), file=sys.stderr)

        if (JobId == analyses[0].job.id):
            InitialSessionAnalysis[session_id] = analyses[0].job.id
            if (debug):
                print("match {} = {}".format(JobId, analyses[0].job.id), file=sys.stderr)

            return(True)

        if (debug):
            print("session {} has initial analysis {} not job {}".format(session_id, analyses[0].job.id,JobId), file=sys.stderr)

    return(False)
     
def getBillableSessionIds(job=None, debug=False):
    BillableSessionIds = {}

    if (isAnalysisJob(job)):
        if (debug):
            print("Job {} is analysis".format(job.id), file=sys.stderr)

        PostEpochAcquisitionIds = getPostEpochAcquisitionIds(job=job, debug=debug)
        if (len(PostEpochAcquisitionIds.keys())):
            if (debug):
                print("PostEpochAcquisitionIds = {}".format(', '.join(PostEpochAcquisitionIds)), file=sys.stderr)
        else:
            if (debug):
                print("job {} has no PostEpochAcquisitionIds".format(job.id), file=sys.stderr)

        for AcquisitionId,AcquisitionDate in PostEpochAcquisitionIds.items():
            SessionId = acquisitionId2SessionId(AcquisitionId)
            if (isJobInitialSessionAnalysis(JobId=job.id, SessionId=SessionId, debug=debug)):
                BillableSessionIds[SessionId] = {
                    'JobId': job.id,
                    'AcquisitionId': AcquisitionId,
                    'AcquisitionDate': str(PostEpochAcquisitionIds[AcquisitionId]),
                    'JobRunDate': job.created,
                }

    else:
        if (debug):
            print("{} not analysis job".format(job.id), file=sys.stderr)

    return(BillableSessionIds)


def initRow(
	job_date=None,
	job_id=None,
        job_elapsed_time_ms=None,
        job_runtime_ms=None,
	gear_id=None,
	gear_name=None,
	gear_version=None,
	gear_category=None,
	job_origin_id=None,
	job_origin_type=None,
	job_state=None,
        job_cpus=None,
        gcp_cpus=None,
        gcp_compute_percent=None,
	initial_analysis=None,
        group=None,
        project=None,
        subject=None,
        session=None,
        session_id=None,
	acquisition_name=None,
	acquisition_id=None,
        gcp_compute_cost=None,
        invoice_month=None,
	job_compute_node_name=None,
    ):

    row = OrderedDict([
	( 'job_date', job_date ),
	( 'job_id', job_id ),
	( 'gear_id', gear_id ),
	( 'gear_name', gear_name ),
	( 'gear_version', gear_version ),
	( 'gear_category', gear_category ),
	( 'job_origin_id', job_origin_id ),
	( 'job_origin_type', job_origin_type ),
	( 'job_state', job_state ),
        ( 'job_elapsed_time_ms', job_elapsed_time_ms ), 
        ( 'job_runtime_ms', job_runtime_ms ), 
        ( 'job_cpus', job_cpus), 
        ( 'gcp_cpus', gcp_cpus), 
        ( 'gcp_compute_percent', gcp_compute_percent),
	( 'initial_analysis', initial_analysis ),
        ( 'group', group ),
        ( 'project', project ),
        ( 'subject', subject ),
        ( 'session', session ),
        ( 'session_id', session_id ),
	( 'acquisition_name', acquisition_name ),
	( 'acquisition_id', acquisition_id ),
        ( 'gcp_compute_cost', gcp_compute_cost),
        ( 'invoice_month', invoice_month),
        ( 'job_compute_node_name', job_compute_node_name ),
    ])
    return(row)

CondorYamlFile = "/home/holder/flywheel/etc/condor.yml"

# jq '.[][][]|select(.engineMatch.whitelist and .engineMatch.whitelist["gear-name"] and (.engineMatch.whitelist["gear-name"][]| match("^fmriprep-phases$"))) | .cloud.machineType' /tmp/json
# 

with open(CondorYamlFile) as f:
    CondorYaml = yaml.load(f, Loader=yaml.FullLoader)

def getJobMachineType(GearName, Tags):
    MachineType = 'n1-standard-1'

    try:
        MachineTypes = pyjq.all('.condor.profiles[]|select(.engineMatch.whitelist and .engineMatch.whitelist["gear-name"] and (.engineMatch.whitelist["gear-name"][]| match("^{}$"))) | .cloud.machineType'.format(GearName), CondorYaml)
        if (len(MachineTypes) > 0):
            MachineType = MachineTypes[0]
        else:
            # *** Should do something about tags here
            True
        return(MachineType)

    except (pyjq.ScriptRuntimeError) as e:
        print("{} GearName failed".format(GearName), file=sys.stderr)

#
# *** Need to handle tags correctly
#

def getJobCpus(job):
    prin
def getJobCpus2(job,detail):
    GearName = detail['gear_info']['name']

    if (re.search('hpc',GearName)):
        cpus = 0
    elif (job.profile.executor and job.profile.executor.cpu_cores > 0):
        cpus = job.profile.executor.cpu_cores 
    else:
        MachineType = getJobMachineType(GearName, job.tags)
        m = re.match('^(?P<class>[^-]+)-(?P<type>[^-]+)-(?P<cores>[^-]+)$',MachineType)
        if (m):
            cpus = int(m.group('cores'))
        else:
            cpus = 0

    return(cpus)

ap = argparse.ArgumentParser()

ap.add_argument('-a', '--augment-cpu-count', action='store_true', help='do extra work to lookup cpu count')
ap.add_argument('-b', '--bar-length', action='store', default=40, type=int, help='bar length')
ap.add_argument('-c', '--gcp-compute-cost', action='store', default=0.0, type=float, help='Total GCP Compute cost for Flywheel ')
ap.add_argument('-d', '--debug', action='store_true', help='debug')

ap.add_argument('-e', '--exhaustive', action='store_false', default=True, help='exhaustive session flag')
ap.add_argument('-E', '--epoch', type=str, help='Epoch date YYYY-MM-DD')
ap.add_argument('-i', '--initial-analysis', action='store_true', help='check for initial analysis flag')
ap.add_argument('-l', '--limit', action='store', default=1000, type=int, help='Flywheel cost allocation csv file')
ap.add_argument('-p', '--progressbar', action='store_true', help='show progress bar')
ap.add_argument('-v', '--verbose', action='store_true', help='verbose')
ap.add_argument('filter', nargs='*', type=str, default=None, help='arguments to fw.find()')

args = ap.parse_args()

if (args.epoch):
    Epoch = parser.parse(str(args.epoch))

UIDs2Labels = {}

fw = fwgearutils.getFW(args, Root=True)

InvoiceMonth = None

filter=','.join(args.filter)
#print("filter = filter={}".format(filter), file=sys.stderr)

s = re.search('^.*>(?P<year>\d{4})-(?P<month>\d{2}).*$',filter)
if (s):
    InvoiceMonth = "{}{}".format(s.group('year'),s.group('month'))
    
#print("InvoiceMonth =", InvoiceMonth)

reapers = fw.get_all_devices()
for r in reapers:
    UIDs2Labels[r.id] = r.name

Gears = {}
gears = fw.get_all_gears()
for g in gears:
    UIDs2Labels[g.id] = g.gear.name
    Gears[g.id] = g

projects = fw.get_all_projects(exhaustive=args.exhaustive, limit=args.limit)
for p in projects:
    UIDs2Labels[p.id] = p.label

subjects = fw.get_all_subjects(exhaustive=args.exhaustive, limit=args.limit)
for s in subjects:
    UIDs2Labels[s.id] = s.label

jobs = []
if (len(args.filter)):
#  jobs = fw.get_all_jobs(limit=args.limit, filter=filter)
  j = fw.jobs.iter_find(*args.filter)
  for i in j:
      jobs.append(i)
else:
  jobs = fw.get_all_jobs(limit=args.limit)


l = len(jobs)
i = 0


Sessions = {}

OutputRows = []
ComputeGCPMs = 0.0

DefaultNodeName = 'upenn-flywheel-site'

print("len(jobs) = '{}'\n".format(len(jobs)), file=sys.stderr)

print("job[0].id = '{}'\n".format(jobs[0].id), file=sys.stderr, flush=True)


for job in jobs:

    job = job.reload()

    if (args.progressbar):
        progress(i, l)

    i += 1
    detail = fw.get_job_detail(job.id)
    if (args.verbose):
        print("job", job, "detail", detail, file=sys.stderr)
    if (not detail.parent_info.project):
        print("detail = ", detail, file=sys.stderr)

    if (args.debug):
        print("({}/{}) job.id = '{} '".format(i,l,job.id), file=sys.stderr, flush=True)

    job_id = job.id
    gear_id = job.gear_id
    job_origin_id = job.origin.id
    job_origin_type = job.origin.type

    session_id = None
    try:
        session_id = detail.parent_info.session.id
    except (AttributeError,TypeError) as e:
        print("job id = '{}' : cannot access detail.parent_info.session.id".format(job.id), detail.parent_info, file=sys.stderr)

    job_origin_id = None
    if (job_origin_id and job_origin_id in UIDs2Labels.keys()):
        job_origin_id = UIDs2Labels[job_origin_id]

    if (getattr(detail.parent_info, 'session') and getattr(detail.parent_info.session, 'id')):
        session_id = detail.parent_info.session.id
    else:
        session_id = None

    acquisition_id = None
    try:
        if (detail.parent_info.acquisition):
            acquisition_id = detail.parent_info.acquisition.id
    except (AttributeError,TypeError) as e:
        print("detail missing acquisition info", detail, file=sys.stderr)

    job_compute_node_name = DefaultNodeName
    if (job.profile and job.profile.executor):
        job_compute_node_name = job['profile']['executor']['name']
    else:
        if (args.debug):
            print("job.id = '{}' : No compute_node_name".format(job.id),file=sys.stderr)

    try:
        cores = job['profile']['executor']['cpu_cores']
    except (AttributeError,TypeError) as e:
        cores = 0

    if(job['profile']['elapsed_time_ms']):
        job_elapsed_time_ms = job['profile']['elapsed_time_ms']
    else:
        job_elapsed_time_ms = 0.0

    if (args.augment_cpu_count):
        gcp_cores = getJobCpus(job, detail)
    elif (re.search('hpc', job.gear_info.name)):
        gcp_cores = 0
    else:
        gcp_cores = cores

    InitialAnalysisCount = 0
    if (args.initial_analysis):
        InitialAnalysisCount = (len(getBillableSessionIds(job=job, debug=args.debug)) > 0)

    if (detail.parent_info.session and detail.parent_info.session.label):
        session_label = re.sub('\s*$','',re.sub('[\\n\\r]',' ',detail.parent_info.session.label))
    else:
        session_label = ""

    try:
        out = initRow(
            job_date=job.created,
            job_id=job.id,
            gear_id=job.gear_id,
            gear_name=job.gear_info.name,
            gear_version=job.gear_info.version,
            gear_category=job.gear_info.category,
            job_compute_node_name=job_compute_node_name,
            job_origin_id=job.origin.id,
            job_origin_type=job.origin.type,
            job_state=job.state,
            job_elapsed_time_ms=job_elapsed_time_ms,
            job_runtime_ms=job['profile']['total_time_ms'],
            job_cpus=cores,
            gcp_cpus=gcp_cores,
            initial_analysis=InitialAnalysisCount,
            group=detail.parent_info.group.id,  #ID is one word lowercase, label can be words and mixed case
            project=detail.parent_info.project.label,
            subject=detail.parent_info.subject.label,
            session=session_label,
            session_id=detail.parent_info.session.id,
            acquisition_id=acquisition_id,
            invoice_month=InvoiceMonth
        )
    except (TypeError) as e:
        print("job.id = '{}' : detail.parent_info.session.label = '{}'".format(job.id,detail.parent_info.session.label), file=sys.stderr)

    if (job['profile']['elapsed_time_ms']):
        ComputeGCPMs += float(gcp_cores) * float(job['profile']['elapsed_time_ms'])

    OutputRows.append(out)

print("len(OutputRows) = ",len(OutputRows), file=sys.stderr)

out = initRow()
writer = csv.DictWriter(sys.stdout, out.keys(),lineterminator='\n' )
writer.writeheader()
for out in OutputRows:

    # if all the jobs are in HCP, gcp ms == 0
    if (ComputeGCPMs):
        out['gcp_compute_percent'] = float(out['gcp_cpus']) * float(out['job_elapsed_time_ms']) / ComputeGCPMs
        out['gcp_compute_cost'] = out['gcp_compute_percent'] * float(args.gcp_compute_cost)

    writer.writerow(out)

#    if (args.verbose):
#        print(json.dumps(fwgearutils.sloppyCopy(session, recurse=True), indent=2), file=sys.stderr)



