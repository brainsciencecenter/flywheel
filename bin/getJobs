#!/usr/bin/python3

#
# get all jobs run from filter
# get the session ids for those analyses
# Get the flywheel paths for those sessions sorting by group/project

import argparse
import csv
import flywheel
import fwgearutils
import json
import os
import re
import sys

# *** Need to add comments to suss out techdev scans

from collections import OrderedDict

CmdName = os.path.basename(sys.argv[0])
Epoch = '2020-01-01'

def progress(count, total, status=''):
    bar_len = 60
    filled_len = int(round(bar_len * count / float(total)))

    percents = round(100.0 * count / float(total), 1)
    bar = '=' * filled_len + '-' * (bar_len - filled_len)

    sys.stderr.write('%s/%s [%s] %s%s ...%s\r' % (i, total, bar, percents, '%', status))
    sys.stderr.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)

# *** nice to have job duration, cpus,  cpu_sec, where it ran, 

InitialSessionAnalysis = {}

def isSessionReaped(session_id=None, debug=False):
    session = fw.get(session_id)
    for a in session.acquisitions():
        try:
            acquisition_origin_id = a.files[0].origin.id
            if (acquisition_origin_id in UIDs2Labels.keys()):
                if (debug):
                    print("id {} from {}".format(acquisition_origin_id, UIDs2Labels[acquisition_origin_id]), file=sys.stderr)
                return(True)

        except (AttributeError) as e:
            True

    return(False)

def isLegacySession(session_id=None, debug=False):
    try:
        session = fw.get(session_id)
    except (flywheel.rest.ApiException) as e:
        # If the session is no longer in flywheel, then it is has been consigned to a Legacy
        return(True)

    if (debug):
        print("session.created.strftime() = {}, Epoch = {}".format(session.created.strftime("%Y-%m-%d"),Epoch), file=sys.stderr)
    if (session.created.strftime("%Y-%m-%d") < Epoch):
        return(True)

    return(False)
        

def isInitialAnalysis(job_id, session_id=None, debug=False):
    
    if (debug):
        print("session_id = {}".format(session_id), file=sys.stderr)

    if (session_id in InitialSessionAnalysis.keys()):
        if (job_id == InitialSessionAnalysis[session_id]):
            print("job_id = {} in InitialSessionAnalysis[{}]".format(job_id,session_id), file=sys.stderr)
            return(not isLegacySession(session_id, debug=debug))

    try:
        jobs = fw.jobs.find("_id={}".format(job_id), limit=100000)
    except (flywheel.rest.ApiException) as e:
        return(False)

    if (len(jobs)):
        job = jobs[0].reload()

        if (debug):
            try:
                print("job.id = {}".format(job.id), file=sys.stderr)
            except (AttributeError) as e:
                print("No job for job[0]", file=sys.stderr)

        detail = fw.get_job_detail(job.id)
        session_id = detail.parent_info.session.id

        try:
            session = fw.get(session_id)
        except (flywheel.rest.ApiException) as e:
            InitialSessionAnalysis[session_id] = None
            print("job_id({})->session.id({}) not found".format(job.id,session_id), file=sys.stderr)
            return(False)

        analyses = sorted(session.analyses, key=lambda a: a.created)
        if (len(analyses)):
            if (job_id == analyses[0].job.id):
                InitialSessionAnalysis[session_id] = analyses[0].job.id
                if (debug):
                    print("match {} = {}".format(job_id, analyses[0].job.id), file=sys.stderr)

                return(not isLegacySession(session_id=session_id, debug=debug))

        if (debug):
            print("Not initial session {} for job {}".format(session_id,job_id), file=sys.stderr)
    else:
        if (debug):
            print("len(jobs) = {} for session id {}".format(len(jobs),session_id), file=sys.stderr)

    return(False)
     
def initRow(
	job_date=None,
	job_id=None,
        job_elapsed_time_ms=None,
        job_runtime_ms=None,
	gear_id=None,
	gear_name=None,
	gear_version=None,
	gear_category=None,
	job_origin_id=None,
	job_origin_type=None,
	job_state=None,
        job_cpus=None,
        gcp_cpus=None,
        gcp_compute_percent=None,
	initial_analysis=None,
        group=None,
        project=None,
        subject=None,
        session=None,
        session_id=None,
	acquisition_name=None,
	acquisition_id=None,
        gcp_compute_cost=None,
    ):

    row = OrderedDict([
	( 'job_date', job_date ),
	( 'job_id', job_id ),
	( 'gear_id', gear_id ),
	( 'gear_name', gear_name ),
	( 'gear_version', gear_version ),
	( 'gear_category', gear_category ),
	( 'job_origin_id', job_origin_id ),
	( 'job_origin_type', job_origin_type ),
	( 'job_state', job_state ),
        ( 'job_elapsed_time_ms', job_elapsed_time_ms ), 
        ( 'job_runtime_ms', job_runtime_ms ), 
        ( 'job_cpus', job_cpus), 
        ( 'gcp_cpus', gcp_cpus), 
        ( 'gcp_compute_percent', gcp_compute_percent),
	( 'initial_analysis', initial_analysis ),
        ( 'group', group ),
        ( 'project', project ),
        ( 'subject', subject ),
        ( 'session', session ),
        ( 'session_id', session_id ),
	( 'acquisition_name', acquisition_name ),
	( 'acquisition_id', acquisition_id ),
        ( 'gcp_compute_cost', gcp_compute_cost),
    ])
    return(row)


ap = argparse.ArgumentParser()

ap.add_argument('-b', '--bar-length', action='store', default=40, type=int, help='bar length')
ap.add_argument('-c', '--gcp-compute-cost', action='store', default=40, type=float, help='$ Cost for Flywheel in GCP Compute')
ap.add_argument('-d', '--debug', action='store_true', help='debug')

ap.add_argument('-e', '--exhaustive', action='store_false', default=True, help='exhaustive session flag')
ap.add_argument('-E', '--epoch', type=str, help='Epoch date YYYY-MM-DD')
ap.add_argument('-i', '--initial-analysis', action='store_true', help='check for initial analysis flag')
ap.add_argument('-l', '--limit', action='store', default=1000000, type=int, help='Flywheel cost allocation csv file')
ap.add_argument('-p', '--progressbar', action='store_true', help='show progress bar')
ap.add_argument('-v', '--verbose', action='store_true', help='verbose')
ap.add_argument('filter', nargs='*', type=str, default=None, help='arguments to fw.find()')

args = ap.parse_args()

if (args.epoch):
    Epoch = args.epoch

UIDs2Labels = {}

fw = fwgearutils.getFW(args, Root=True)

reapers = fw.get_all_devices()
for r in reapers:
    UIDs2Labels[r.id] = r.name

Gears = {}
gears = fw.get_all_gears()
for g in gears:
    UIDs2Labels[g.id] = g.gear.name
    Gears[g.id] = g

projects = fw.get_all_projects(exhaustive=args.exhaustive, limit=args.limit)
for p in projects:
    UIDs2Labels[p.id] = p.label

subjects = fw.get_all_subjects(exhaustive=args.exhaustive, limit=args.limit)
for s in subjects:
    UIDs2Labels[s.id] = s.label

filter=','.join(args.filter)
print("filter = filter={}".format(filter), file=sys.stderr)
jobs = []
if (len(args.filter)):
#  jobs = fw.get_all_jobs(limit=args.limit, filter=filter)
  j = fw.jobs.iter_find(*args.filter)
  for i in j:
      jobs.append(i)
else:
  jobs = fw.get_all_jobs(limit=args.limit)

print("Len jobs = ",len(jobs), file=sys.stderr)

l = len(jobs)
i = 0


Sessions = {}

OutputRows = []
ComputeGCPMs = 0.0

for job in jobs:
    job = job.reload()

    if (args.progressbar):
        progress(i, l)
        i += 1

    detail = fw.get_job_detail(job.id)
    if (args.verbose):
        print("job", job, "detail", detail, file=sys.stderr)

    try:
        job_id = job.id
        gear_id = job.gear_id
        job_origin_id = job.origin.id
        job_origin_type = job.origin.type

        if (getattr(detail.parent_info, 'session') and getattr(detail.parent_info.session, 'id')):
            session_id = detail.parent_info.session.id
        else:
            session_id = None

        if (job_origin_id and job_origin_id in UIDs2Labels.keys()):
            job_origin_id = UIDs2Labels[job_origin_id]

        if (getattr(detail.parent_info, 'session') and getattr(detail.parent_info.session, 'id')):
            session_id = detail.parent_info.session.id
        else:
            session_id = None

        if (getattr(detail.parent_info, 'acquisition') and getattr(detail.parent_info.acquisition, 'id')):
            acquisition_id = detail.parent_info.acquisition.id
        else:
            acquisition_id = None
            # print("detail missing acquisition info", detail, file=sys.stderr)

        try:
            cores = job['profile']['executor']['cpu_cores']
        except (AttributeError,TypeError) as e:
            cores = 0

        if(job['profile']['elapsed_time_ms']):
            job_elapsed_time_ms = job['profile']['elapsed_time_ms']
        else:
            job_elapsed_time_ms = 0.0
            
        if (re.search('hpc', job.gear_info.name)):
            gcp_cores = 0
        else:
            gcp_cores = cores

        InitialAnalysis = None
        if (args.initial_analysis):
            try:
                InitialAnalysis = isInitialAnalysis(job.id, session_id=session_id, debug=args.debug)
            except (AttributeError) as e:
                True

        if (detail.parent_info.session.label):
            session_label = re.sub('\s*$','',re.sub('[\\n\\r]',' ',detail.parent_info.session.label))
        else:
            session_label = ""

        try:
            out = initRow(
                job_date=job.created,
                job_id=job.id,
                gear_id=job.gear_id,
                gear_name=job.gear_info.name,
                gear_version=job.gear_info.version,
                gear_category=job.gear_info.category,
                job_origin_id=job.origin.id,
                job_origin_type=job.origin.type,
                job_state=job.state,
                job_elapsed_time_ms=job_elapsed_time_ms,
                job_runtime_ms=job['profile']['total_time_ms'],
                job_cpus=cores,
                gcp_cpus=gcp_cores,
                initial_analysis=InitialAnalysis,
                group=detail.parent_info.group.id,  #ID is one word lowercase, label can be words and mixed case
                project=detail.parent_info.project.label,
                subject=detail.parent_info.subject.label,
                session=session_label,
                session_id=detail.parent_info.session.id,
                acquisition_id=acquisition_id
            )
        except (TypeError) as e:
            print("detail.parent_info.session.label = '{}'".format(detail.parent_info.session.label), file=sys.stderr)

        if (job['profile']['elapsed_time_ms']):
            ComputeGCPMs += float(gcp_cores) * float(job['profile']['elapsed_time_ms'])

        OutputRows.append(out)

    except (AttributeError) as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        print(exc_type, fname, exc_tb.tb_lineno, file=sys.stderr)

print("len(OutputRows) = ",len(OutputRows), file=sys.stderr)

out = initRow()
writer = csv.DictWriter(sys.stdout, out.keys(),lineterminator='\n' )
writer.writeheader()
for out in OutputRows:

    # if all the jobs are in HCP, gcp ms == 0
    if (ComputeGCPMs):
        out['gcp_compute_percent'] = float(out['gcp_cpus']) * float(out['job_elapsed_time_ms']) / ComputeGCPMs
    out['gcp_compute_cost'] = out['gcp_compute_percent'] * float(args.gcp_compute_cost)

    writer.writerow(out)

#    if (args.verbose):
#        print(json.dumps(fwgearutils.sloppyCopy(session, recurse=True), indent=2), file=sys.stderr)



