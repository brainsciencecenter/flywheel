#!/bin/bash

#
#
# *** Should get rid of group level prorating.  Gcp group costs are a good check, but prorating should be across
#     the total metrics to simplify the tables and reduce the chance of getting the prorating wrong

#
# Table explaination
#
# Inputs

# GcpClassifiedMetrics
#    Google billing information broken out into all the different categories we care, or might care about
#    Each row is a single billing event.
#    Some events have a google tag correspointing to a flywheel group.  Some billing events do not have flywheel group
#    tags.  These are mostly for non-group related things, like data redundancy, inter-VM, or inter regiion data flows etc.
#    Within the tagged items, the majority are related to running the flywheel upenn site, web site, database, handling
#    VM startups.
#    One other weirdness, is flywheel allows all numeric groups, but google tags are required to start with a lowercase letter.
#    Flywheel groups (eg group 13 - Nasa) is prefixed with a leading 'g-' so it is tagged in google as 'g-13'.
#
#    The major GCP classifications are compute, storage, and misc.  Within compute and storage, we're interested in dynamic, static,
#    analysis and utility (*** do we still care about the analysis/utility split, don't they fall out with the gear type?)
#
#    GcpClassifiedMetrics has a further problem in that all storage costs are recorded against the flywheel-static (site) group.
#    These charges have to be broken out into the gears that ran on the site (recorded in the FwComputeMetrics table) and the overhead
#
# FwComputeMetrics
#    This data is pulled from the flywheel job logs.
#    Each row is a single gear run and includes the group/project/gearname/geartype and whether the gear was run
#    on the site VM (static) or on a group tagged VM (dynamic)
#    Each row includes the number of CPU hours flywheel thinks the gear ran, and seems to include VM startup time, as well as
#    Whether this is the first analysis run on the session.  May no longer care about this metric.
#    The flywheel metrics have only marginal correlation with the google charges.  This is due to multiple gears being run
#    on the same VM as well as the startup/teardown times for the VMs.
#    As a result, the flywheel metrics are only used to allocate the google metrics, not a direct reflection.
#    So if flwheel says the dicom classifier under dwolklab/NACC-SC used 500 CPU hours s out of a total of 1000 CPU hours 
#    flywheel recorded, and google charged us $50 for 100 CPU hours for dwolklab, then 500/1000 * $50 in costs will be allocated
#    to the dwolklab/NACC-SC/dicom-classifier gear and 500/1000 * 500 CPU hours.
#
#  FwStorageMetrics
#    Storage metrics are loaded from the flywheel usage reports for the invoice month.
#
#    Again, the flywheel metrics are corellated with the google billing metrics, but only within an order a few percent.
#    This has to do with the way flywheel calculates the storage, and doesn't provide any metrics for coldline, or archival storage
#    as well as having no records of what got deleted or when.
#
#    The flywheel usage report doesn't include whether or when data was deleted, so with each line from the usage report
#    we have to check whether the group/project the storage was recorded against still exists in the upenn flywheel site.
#    In theory, a group could set up a project and delete it at the end of the month and re-upload after the billing was run to
#    escape being charged.
#
#  FwComputeStorageMetrics
#    This is a full outer join of the FwCompute and FwStorage tables.
#
#    The full outer join ensures we get all the group/project pairs in the event that a group/project pair didn't have compute,
#    or storage one month.  There are many times when a group/project has left over VM system disks in GCP which get charged to
#    the group, but the group didn't run any gears.  It's harder to have gears without storage, but you can have gears run, and
#    then delete the storage.
#
#    *** We should probably just ignore all deleted storage
#
#    Because the storgage table rows are all based on group/project and the compute table is based on group/project/gear
#    there isn't a good way to combine the storage and compute rows, so a BogusStorageGear is created to hold all
#    the storage charges for the group/project
#
#
#  GcpFwComputeStorageMetrics
#
#  GcpFwGroupComputeStorageMetrics
#    This is a full outer join of the GcpClassifiedMetrics table with the sums by group in the FwComputeStorageMetircs table.
#    BogusGroup/BogusProject/BogusComputeGear and BogusStorageGears are created when the join generates nulls.
#    The totals are used to allocate metrics and charges to the gears in a later table.
#
#  GcpFwTotalComputeStorageMetrics
#    Totals for all the GCP classifications and Flywheel metrics we need to allocate metrics and charges across groups.
#    Compute costs can mostly be allocated within a group since we have most of the charges from Google with a group tag.
#    All the storage costs are tagged with flywheel-static so we have to allocate those costs from sums across all groups, being
#    careful for some measures whose totals need to not include flywheel-static.
#
#    Should be one row / InvoiceMonth
#
#    The static gear metric must be calculated which will reduce the overhead.  All the gears run for projects on the site node
#    are mis-labeled by google.  We can calculate a Flywheel to Google hours ratio, then use that to estimate how much of the 
#    flywheel-static hours and costs should be allocated to individual projects instead of to general overhead.
#
#  AllocatedGcpFwGroupProjectGears
#    Allocations based on group level metrics
#
#  AllocatedGcpFwProjectGears
#    Allocations based on site level metrics
#
#  AllocatedProjectCharges
#    Calulate the BSC charges for each project based on the metrics and billing algorithms
#     
# *** How do we deal with entire groups which have been deleted by the time we run the reports?

# *** Have to make sure every table is filtered through InvoiceMonth including the joined tables


CmdName=$(basename "$0")

syntax="${CmdName} [-d Dataset][-f][-p Project][-v] {-m Month} {-y Year}"

Version=5

function sys {
    [ -n "${opt_n}${opt_v}" ] && echo "$@" 1>&2
    [ -n "$opt_n" ] || "$@"
}

function OutputControl {
    local Verbose="$1"
    
    if [ -n "$opt_v" ] || [ -n "Verbose" ]
    then
	cat
    else
	cat > /dev/null
    fi
}

while getopts d:fm:p:vy: arg
do
	case "$arg" in
		d|f|m|p|v|y)
			eval "opt_${arg}=${OPTARG:=1}"
			;;
	esac
done

shift $(($OPTIND - 1))

CommonArgs=()

if [ -z "$opt_y" ] || [ -z "$opt_m" ]
then
	echo "$syntax" 1>&2
	exit 1
fi

Project=pennbrain-center

[ -n "$opt_d" ] && CommonArgs+=( -d "$opt_d" )

[ -n "$opt_f" ] && CommonArgs+=( -f )

[ -n "$opt_m" ] && CommonArgs+=( -m "$opt_m" )

[ -n "$opt_p" ] && CommonArgs+=( -p "$opt_p" )

[ -n "$opt_v" ] && CommonArgs+=( -v )

[ -n "$opt_y" ] && CommonArgs+=( -y "$opt_y" )


sys v5GcpClassifiedMetrics "${CommonArgs[@]}"
sys v5FwComputeMetrics  "${CommonArgs[@]}"
sys v5FwStorageMetrics "${CommonArgs[@]}"
sys v5FwComputeStorageMetrics "${CommonArgs[@]}"

sys v5GcpFwGroupComputeStorageMetrics "${CommonArgs[@]}"

sys v5GcpFwGroupTotalComputeStorageMetrics "${CommonArgs[@]}"
sys v5GcpFwTotalComputeStorageMetrics "${CommonArgs[@]}"

sys v5AllocatedGcpFwGroupProjectGears "${CommonArgs[@]}"

sys v5EstTotalStaticGearOverheadMetrics "${CommonArgs[@]}"

sys v5AllocatedStaticGearSiteOverhead "${CommonArgs[@]}"

sys v5CalculateBscCharges "${CommonArgs[@]}"

